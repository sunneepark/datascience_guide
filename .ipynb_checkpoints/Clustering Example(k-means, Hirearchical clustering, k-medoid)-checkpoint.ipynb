{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from linear_algebra import squared_distance, vector_mean, distance\n",
    "import math, random\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    \"\"\"performs k-means clustering\"\"\"\n",
    "\n",
    "    def __init__(self, k):\n",
    "        self.k = k          # number of clusters\n",
    "        self.means = None   # means of clusters\n",
    "        \n",
    "    def classify(self, input):\n",
    "        \"\"\"return the index of the cluster closest to the input\"\"\"\n",
    "        return min(range(self.k),\n",
    "                   key=lambda i: squared_distance(input, self.means[i]))\n",
    "                   \n",
    "    def train(self, inputs):\n",
    "    \n",
    "        self.means = random.sample(inputs, self.k)\n",
    "        assignments = None\n",
    "        \n",
    "        while True:\n",
    "            # Find new assignments\n",
    "            new_assignments = map(self.classify, inputs)\n",
    "\n",
    "            # If no assignments have changed, we're done.\n",
    "            if assignments == new_assignments:                \n",
    "                return\n",
    "\n",
    "            # Otherwise keep the new assignments,\n",
    "            assignments = new_assignments    \n",
    "\n",
    "            for i in range(self.k):\n",
    "                i_points = [p for p, a in zip(inputs, assignments) if a == i]\n",
    "                # avoid divide-by-zero if i_points is empty\n",
    "                if i_points:                                \n",
    "                    self.means[i] = vector_mean(i_points)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_clustering_errors(inputs, k):\n",
    "    \"\"\"finds the total squared error from k-means clustering the inputs\"\"\"\n",
    "    clusterer = KMeans(k)\n",
    "    clusterer.train(inputs)\n",
    "    means = clusterer.means\n",
    "    assignments = map(clusterer.classify, inputs)\n",
    "    \n",
    "    return sum(squared_distance(input,means[cluster])\n",
    "               for input, cluster in zip(inputs, assignments))\n",
    "\n",
    "def plot_squared_clustering_errors(plt):\n",
    "\n",
    "    ks = range(1, len(inputs) + 1)\n",
    "    errors = [squared_clustering_errors(inputs, k) for k in ks]\n",
    "\n",
    "    plt.plot(ks, errors)\n",
    "    plt.xticks(ks)\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"total squared error\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# using clustering to recolor an image\n",
    "#\n",
    "\n",
    "def recolor_image(input_file, k=5):\n",
    "\n",
    "    img = mpimg.imread(path_to_png_file)\n",
    "    pixels = [pixel for row in img for pixel in row]\n",
    "    clusterer = KMeans(k)\n",
    "    clusterer.train(pixels) # this might take a while    \n",
    "\n",
    "    def recolor(pixel):\n",
    "        cluster = clusterer.classify(pixel) # index of the closest cluster\n",
    "        return clusterer.means[cluster]     # mean of the closest cluster\n",
    "\n",
    "    new_img = [[recolor(pixel) for pixel in row]\n",
    "               for row in img]\n",
    "\n",
    "    plt.imshow(new_img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# hierarchical clustering\n",
    "#\n",
    "\n",
    "def is_leaf(cluster):\n",
    "    \"\"\"a cluster is a leaf if it has length 1\"\"\"\n",
    "    return len(cluster) == 1\n",
    "\n",
    "def get_children(cluster):\n",
    "    \"\"\"returns the two children of this cluster if it's a merged cluster;\n",
    "    raises an exception if this is a leaf cluster\"\"\"\n",
    "    if is_leaf(cluster):\n",
    "        raise TypeError(\"a leaf cluster has no children\")\n",
    "    else:\n",
    "        return cluster[1]\n",
    "\n",
    "def get_values(cluster):\n",
    "    \"\"\"returns the value in this cluster (if it's a leaf cluster)\n",
    "    or all the values in the leaf clusters below it (if it's not)\"\"\"\n",
    "    if is_leaf(cluster):\n",
    "        return cluster # is already a 1-tuple containing value\n",
    "    else:\n",
    "        return [value\n",
    "                for child in get_children(cluster)\n",
    "                for value in get_values(child)]\n",
    "\n",
    "def cluster_distance(cluster1, cluster2, distance_agg=min):\n",
    "    \"\"\"finds the aggregate distance between elements of cluster1\n",
    "    and elements of cluster2\"\"\"\n",
    "    return distance_agg([distance(input1, input2)\n",
    "                        for input1 in get_values(cluster1)\n",
    "                        for input2 in get_values(cluster2)])\n",
    "\n",
    "def get_merge_order(cluster):\n",
    "    if is_leaf(cluster):\n",
    "        return float('inf')\n",
    "    else:\n",
    "        return cluster[0] # merge_order is first element of 2-tuple\n",
    "\n",
    "def bottom_up_cluster(inputs, distance_agg=min):\n",
    "    # start with every input a leaf cluster / 1-tuple\n",
    "    clusters = [(input,) for input in inputs]\n",
    "    \n",
    "    # as long as we have more than one cluster left...\n",
    "    while len(clusters) > 1:\n",
    "        # find the two closest clusters\n",
    "        c1, c2 = min([(cluster1, cluster2)\n",
    "                     for i, cluster1 in enumerate(clusters)\n",
    "                     for cluster2 in clusters[:i]],\n",
    "                     key=lambda x, y: cluster_distance(x, y, distance_agg))\n",
    "\n",
    "        # remove them from the list of clusters\n",
    "        clusters = [c for c in clusters if c != c1 and c != c2]\n",
    "\n",
    "        # merge them, using merge_order = # of clusters left\n",
    "        merged_cluster = (len(clusters), [c1, c2])\n",
    "\n",
    "        # and add their merge\n",
    "        clusters.append(merged_cluster)\n",
    "\n",
    "    # when there's only one cluster left, return it\n",
    "    return clusters[0]\n",
    "\n",
    "def generate_clusters(base_cluster, num_clusters):\n",
    "    # start with a list with just the base cluster\n",
    "    clusters = [base_cluster]\n",
    "    \n",
    "    # as long as we don't have enough clusters yet...\n",
    "    while len(clusters) < num_clusters:\n",
    "        # choose the last-merged of our clusters\n",
    "        next_cluster = min(clusters, key=get_merge_order)\n",
    "        # remove it from the list\n",
    "        clusters = [c for c in clusters if c != next_cluster]\n",
    "        # and add its children to the list (i.e., unmerge it)\n",
    "        clusters.extend(get_children(next_cluster))\n",
    "\n",
    "    # once we have enough clusters...\n",
    "    return clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-means 클러스터링 k=2 , k=3 알고리즘 예시 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [[-14,-5],[13,13],[20,23],[-19,-11],[-9,-16],[21,27],[-49,15],[26,13],[-46,5],[-34,-1],[11,15],[-49,0],[-22,-16],[19,28],[-12,-8],[-13,-19],[-41,8],[-11,-6],[-25,-9],[-18,-3]]\n",
    "\n",
    "random.seed(0) # so you get the same results as me\n",
    "clusterer = KMeans(3)\n",
    "clusterer.train(inputs)\n",
    "\n",
    "print (\"3-means:\")\n",
    "print (clusterer.means)\n",
    "print (\"\\n\")\n",
    "random.seed(0)\n",
    "\n",
    "clusterer = KMeans(2)\n",
    "clusterer.train(inputs)\n",
    "print (\"2-means:\")\n",
    "print (clusterer.means)\n",
    "print (\"\\n\")\n",
    "\n",
    "print (\"errors as a function of k\")\n",
    "\n",
    "for k in range(1, len(inputs) + 1):\n",
    "    print (k, squared_clustering_errors(inputs, k),sep=' ')\n",
    "print (\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hierarchical 클러스터링 알고리즘 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"bottom up hierarchical clustering\")\n",
    "\n",
    "base_cluster = bottom_up_cluster(inputs)\n",
    "print (base_cluster)\n",
    "\n",
    "print (\"\\n\")\n",
    "print (\"three clusters, min:\")\n",
    "for cluster in generate_clusters(base_cluster, 3):\n",
    "    print (get_values(cluster))\n",
    "\n",
    "print (\"\\n\")\n",
    "print (\"three clusters, max:\")\n",
    "base_cluster = bottom_up_cluster(inputs, max)\n",
    "for cluster in generate_clusters(base_cluster, 3):\n",
    "    print (get_values(cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-medoid 클러스터링 알고리즘 예시 -> 실제 결과와의 매치율까지 확인\n",
    "#https://towardsdatascience.com/k-medoids-clustering-on-iris-data-set-1931bf781e05 참조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Dataset\n",
    "iris = datasets.load_iris()\n",
    "data = pd.DataFrame(iris.data,columns = iris.feature_names)\n",
    "\n",
    "target = iris.target_names\n",
    "labels = iris.target\n",
    "\n",
    "#Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "data = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n",
    "\n",
    "#PCA Transformation\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "principalComponents = pca.fit_transform(data)\n",
    "PCAdf = pd.DataFrame(data = principalComponents , columns = ['principal component 1', 'principal component 2','principal component 3'])\n",
    "\n",
    "datapoints = PCAdf.values\n",
    "m, f = datapoints.shape\n",
    "k = 3\n",
    "\n",
    "#Visualization\n",
    "fig = plt.figure(1, figsize=(8, 6))\n",
    "ax = Axes3D(fig, elev=-150, azim=110)\n",
    "X_reduced = points\n",
    "ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=labels,\n",
    "           cmap=plt.cm.Set1, edgecolor='k', s=40)\n",
    "ax.set_title(\"First three PCA directions\")\n",
    "ax.set_xlabel(\"principal component 1\")\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.set_ylabel(\"principal component 1\")\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.set_zlabel(\"principal component 1\")\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "plt.show()\n",
    "\n",
    "def init_medoids(X, k):\n",
    "    from numpy.random import choice\n",
    "    from numpy.random import seed\n",
    " \n",
    "    seed(1)\n",
    "    samples = choice(len(X), size=k, replace=False)\n",
    "    return X[samples, :]\n",
    "\n",
    "medoids_initial = init_medoids(datapoints, 3)\n",
    "\n",
    "def compute_d_p(X, medoids, p):\n",
    "    m = len(X)\n",
    "    medoids_shape = medoids.shape\n",
    "    # If a 1-D array is provided, \n",
    "    # it will be reshaped to a single row 2-D array\n",
    "    if len(medoids_shape) == 1: \n",
    "        medoids = medoids.reshape((1,len(medoids)))\n",
    "    k = len(medoids)\n",
    "    \n",
    "    S = np.empty((m, k))\n",
    "    \n",
    "    for i in range(m):\n",
    "        d_i = np.linalg.norm(X[i, :] - medoids, ord=p, axis=1)\n",
    "        S[i, :] = d_i**p\n",
    "\n",
    "    return S\n",
    "  \n",
    "S = compute_d_p(datapoints, medoids_initial, 2)\n",
    "\n",
    "\n",
    "def assign_labels(S):\n",
    "    return np.argmin(S, axis=1)\n",
    "  \n",
    "labels = assign_labels(S)\n",
    "\n",
    "def update_medoids(X, medoids, p):\n",
    "    \n",
    "    S = compute_d_p(points, medoids, p)\n",
    "    labels = assign_labels(S)\n",
    "        \n",
    "    out_medoids = medoids\n",
    "                \n",
    "    for i in set(labels):\n",
    "        \n",
    "        avg_dissimilarity = np.sum(compute_d_p(points, medoids[i], p))\n",
    "\n",
    "        cluster_points = points[labels == i]\n",
    "        \n",
    "        for datap in cluster_points:\n",
    "            new_medoid = datap\n",
    "            new_dissimilarity= np.sum(compute_d_p(points, datap, p))\n",
    "            \n",
    "            if new_dissimilarity < avg_dissimilarity :\n",
    "                avg_dissimilarity = new_dissimilarity\n",
    "                \n",
    "                out_medoids[i] = datap\n",
    "                \n",
    "    return out_medoids\n",
    "\n",
    "def has_converged(old_medoids, medoids):\n",
    "    return set([tuple(x) for x in old_medoids]) == set([tuple(x) for x in medoids])\n",
    "  \n",
    "#Full algorithm\n",
    "def kmedoids(X, k, p, starting_medoids=None, max_steps=np.inf):\n",
    "    if starting_medoids is None:\n",
    "        medoids = init_medoids(X, k)\n",
    "    else:\n",
    "        medoids = starting_medoids\n",
    "        \n",
    "    converged = False\n",
    "    labels = np.zeros(len(X))\n",
    "    i = 1\n",
    "    while (not converged) and (i <= max_steps):\n",
    "        old_medoids = medoids.copy()\n",
    "        \n",
    "        S = compute_d_p(X, medoids, p)\n",
    "        \n",
    "        labels = assign_labels(S)\n",
    "        \n",
    "        medoids = update_medoids(X, medoids, p)\n",
    "        \n",
    "        converged = has_converged(old_medoids, medoids)\n",
    "        i += 1\n",
    "    return (medoids,labels)\n",
    "\n",
    "results = kmedoids(datapoints, 3, 2)\n",
    "final_medoids = results[0]\n",
    "data['clusters'] = results[1]\n",
    "\n",
    "#Count\n",
    "def mark_matches(a, b, exact=False):\n",
    "    \"\"\"\n",
    "    Given two Numpy arrays of {0, 1} labels, returns a new boolean\n",
    "    array indicating at which locations the input arrays have the\n",
    "    same label (i.e., the corresponding entry is True).\n",
    "    \n",
    "    This function can consider \"inexact\" matches. That is, if `exact`\n",
    "    is False, then the function will assume the {0, 1} labels may be\n",
    "    regarded as the same up to a swapping of the labels. This feature\n",
    "    allows\n",
    "    \n",
    "      a == [0, 0, 1, 1, 0, 1, 1]\n",
    "      b == [1, 1, 0, 0, 1, 0, 0]\n",
    "      \n",
    "    to be regarded as equal. (That is, use `exact=False` when you\n",
    "    only care about \"relative\" labeling.)\n",
    "    \"\"\"\n",
    "    assert a.shape == b.shape\n",
    "    a_int = a.astype(dtype=int)\n",
    "    b_int = b.astype(dtype=int)\n",
    "    all_axes = tuple(range(len(a.shape)))\n",
    "    assert ((a_int == 0) | (a_int == 1) | (a_int == 2)).all()\n",
    "    assert ((b_int == 0) | (b_int == 1) | (b_int == 2)).all()\n",
    "    \n",
    "    exact_matches = (a_int == b_int)\n",
    "    if exact:\n",
    "        return exact_matches\n",
    "\n",
    "    assert exact == False\n",
    "    num_exact_matches = np.sum(exact_matches)\n",
    "    if (2*num_exact_matches) >= np.prod (a.shape):\n",
    "        return exact_matches\n",
    "    return exact_matches == False # Invert\n",
    "\n",
    "def count_matches(a, b, exact=False):\n",
    "    \"\"\"\n",
    "    Given two sets of {0, 1} labels, returns the number of mismatches.\n",
    "    \n",
    "    This function can consider \"inexact\" matches. That is, if `exact`\n",
    "    is False, then the function will assume the {0, 1} labels may be\n",
    "    regarded as similar up to a swapping of the labels. This feature\n",
    "    allows\n",
    "    \n",
    "      a == [0, 0, 1, 1, 0, 1, 1]\n",
    "      b == [1, 1, 0, 0, 1, 0, 0]\n",
    "      \n",
    "    to be regarded as equal. (That is, use `exact=False` when you\n",
    "    only care about \"relative\" labeling.)\n",
    "    \"\"\"\n",
    "    matches = mark_matches(a, b, exact=exact)\n",
    "    return np.sum(matches)\n",
    "\n",
    "n_matches = count_matches(labels, data['clusters'])\n",
    "print(n_matches,\n",
    "      \"matches out of\",\n",
    "      len(data), \"data points\",\n",
    "      \"(~ {:.1f}%)\".format(100.0 * n_matches / len(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
